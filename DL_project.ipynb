{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFcuI55rSgsLMcTiOW0BEK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leoroman1/DL_project/blob/main/DL_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJYPASMc94rC",
        "outputId": "60f2e679-1b04-4d3b-80eb-c17b8ed5663c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# connect to your drive with the dataset folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the paths of the tar dataset and the destination of the extracted dataset ONLY IF NEEDED\n",
        "\n",
        "tar_path = '/content/drive/MyDrive/Task01_BrainTumour.tar'\n",
        "extract_path = '/content/drive/MyDrive/Task01_BrainTumour'\n"
      ],
      "metadata": {
        "id": "zw9ImmfC-B_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset tar extraction ONLY IF NEEDED\n",
        "\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Paths (modify as needed)\n",
        "tar_path = '/content/drive/MyDrive/Task01_BrainTumour.tar'\n",
        "extract_path = '/content/drive/MyDrive/Task01_BrainTumour'\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Open and extract with progress bar\n",
        "with tarfile.open(tar_path) as tar:\n",
        "    members = tar.getmembers()\n",
        "    print(f\"Extracting {len(members)} files to: {extract_path}\")\n",
        "    for member in tqdm(members, desc=\"Extracting\", unit=\"file\"):\n",
        "        tar.extract(member, path=extract_path)\n",
        "\n",
        "print(\"✅ Extraction complete.\")\n"
      ],
      "metadata": {
        "id": "woXdVyyO-bfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/Task01_BrainTumour/Task01_BrainTumour'  # change if needed\n",
        "image_dir = f'{data_dir}/imagesTr'\n",
        "label_dir = f'{data_dir}/labelsTr'"
      ],
      "metadata": {
        "id": "eEavKDO6-74W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a function to load the image and label volumes from .nii.gz files\n",
        "def load_volume(img_path, lbl_path):\n",
        "    # Load the image data using nibabel and get the numpy array\n",
        "    img = nib.load(img_path).get_fdata()   # Expected shape: (Height, Width, Depth, Modalities=4)\n",
        "    # Load the label data using nibabel and get the numpy array\n",
        "    lbl = nib.load(lbl_path).get_fdata()   # Expected shape: (Height, Width, Depth)\n",
        "    # Convert the image data to float32 and the label data to uint8 for consistency\n",
        "    return img.astype(np.float32), lbl.astype(np.uint8)\n",
        "\n",
        "# Define the file paths for a sample image and its corresponding label\n",
        "sample_image = f'{image_dir}/BRATS_001.nii.gz'\n",
        "sample_label = f'{label_dir}/BRATS_001.nii.gz'\n",
        "\n",
        "# Load the sample image and label using the defined function\n",
        "x, y = load_volume(sample_image, sample_label)\n",
        "\n",
        "# Print the shapes of the loaded image and label arrays\n",
        "print(\"Image shape:\", x.shape)\n",
        "print(\"Label shape:\", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFuZZSDQ_18K",
        "outputId": "ebcd266c-7f6c-457d-fc47-7e428593065b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: (240, 240, 155, 4)\n",
            "Label shape: (240, 240, 155)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create figure for modalities\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Loop through modalities\n",
        "for i in range(4):\n",
        "    # Create subplot\n",
        "    plt.subplot(1, 4, i+1)\n",
        "    # Display middle slice of current modality\n",
        "    plt.imshow(x[..., x.shape[2]//2, i], cmap='gray')\n",
        "    # Set modality title\n",
        "    plt.title(f'Modality {i+1}')\n",
        "\n",
        "# Show modality plots\n",
        "plt.show()\n",
        "\n",
        "# Display middle slice of label\n",
        "plt.imshow(y[..., y.shape[2]//2], cmap='nipy_spectral')\n",
        "# Set label title\n",
        "plt.title('Label')\n",
        "\n",
        "# Show label plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xwykUq1UAHm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt                  # For plotting\n",
        "import ipywidgets as widgets                    # For interactive sliders in Jupyter\n",
        "from IPython.display import display             # To display widgets in output cell\n",
        "\n",
        "def show_mri_with_labels(volume, label, modality=0, axis=2, alpha=0.4):\n",
        "    \"\"\"\n",
        "    Interactive visualization of MRI slices with segmentation labels.\n",
        "\n",
        "    Parameters:\n",
        "    - volume: 4D numpy array (H, W, D, modalities)\n",
        "    - label: 3D numpy array (H, W, D) with segmentation labels\n",
        "    - modality: index of the modality to visualize\n",
        "    - axis: axis along which to slice (0, 1, or 2)\n",
        "    - alpha: transparency level for label overlay\n",
        "    \"\"\"\n",
        "    assert volume.shape[:3] == label.shape, \"Volume and label shape mismatch\"\n",
        "    max_idx = volume.shape[axis] - 1  # Get the number of slices along chosen axis\n",
        "\n",
        "    def view_slice(i):\n",
        "        plt.figure(figsize=(6, 6))  # Set figure size\n",
        "\n",
        "        # Extract the image and label slice based on selected axis\n",
        "        if axis == 0:\n",
        "            img = volume[i, :, :, modality]\n",
        "            lbl = label[i, :, :]\n",
        "        elif axis == 1:\n",
        "            img = volume[:, i, :, modality]\n",
        "            lbl = label[:, i, :]\n",
        "        else:\n",
        "            img = volume[:, :, i, modality]\n",
        "            lbl = label[:, :, i]\n",
        "\n",
        "        # Display grayscale image with label overlay\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.imshow(lbl, cmap='nipy_spectral', alpha=alpha)  # Overlay labels with transparency\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Slice {i} — Modality {modality}')\n",
        "        plt.show()\n",
        "\n",
        "    # Create an interactive slider to scroll through slices\n",
        "    slider = widgets.IntSlider(min=0, max=max_idx, step=1, value=max_idx // 2)\n",
        "    display(widgets.interact(view_slice, i=slider))  # Link slider to view_slice function\n",
        "\n",
        "# Example usage:\n",
        "show_mri_with_labels(x, y, modality=0, axis=2)  # Visualize modality 0 along the axial axis\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LjYlzlJcAaZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === SETUP ===\n",
        "\n",
        "import os, glob\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === DATASET CLASS ===\n",
        "\n",
        "class BraTSDataset(Dataset):\n",
        "    def __init__(self, image_paths, label_paths, patch_size=(128, 128, 72)):\n",
        "        self.image_paths = image_paths\n",
        "        self.label_paths = label_paths\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = nib.load(self.image_paths[idx]).get_fdata().astype(np.float32)  # (H, W, D, 4)\n",
        "        lbl = nib.load(self.label_paths[idx]).get_fdata().astype(np.uint8)    # (H, W, D)\n",
        "\n",
        "        # Center crop\n",
        "        H, W, D = lbl.shape\n",
        "        ph, pw, pd = self.patch_size\n",
        "        sh = (H - ph) // 2\n",
        "        sw = (W - pw) // 2\n",
        "        sd = (D - pd) // 2\n",
        "\n",
        "        img = img[sh:sh+ph, sw:sw+pw, sd:sd+pd, :]\n",
        "        lbl = lbl[sh:sh+ph, sw:sw+pw, sd:sd+pd]\n",
        "\n",
        "        # Convert to PyTorch tensors: (C, H, W, D) and (H, W, D)\n",
        "        img = torch.tensor(img).permute(3, 0, 1, 2)\n",
        "        lbl = torch.tensor(lbl, dtype=torch.long)\n",
        "        return img, lbl\n",
        "\n",
        "# === SIMPLE 3D UNET ===\n",
        "\n",
        "class Simple3DUNet(nn.Module):\n",
        "    def __init__(self, in_channels=4, out_classes=4):\n",
        "        super().__init__()\n",
        "        self.enc1 = self.conv_block(in_channels, 16)\n",
        "        self.enc2 = self.conv_block(16, 32)\n",
        "        self.pool = nn.MaxPool3d(2)\n",
        "\n",
        "        self.bottleneck = self.conv_block(32, 64)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)\n",
        "        self.dec1 = self.conv_block(64, 32)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose3d(32, 16, kernel_size=2, stride=2)\n",
        "        self.dec2 = self.conv_block(32, 16)\n",
        "\n",
        "        self.out_conv = nn.Conv3d(16, out_classes, kernel_size=1)\n",
        "\n",
        "    def conv_block(self, in_ch, out_ch):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv3d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool(e1))\n",
        "        b = self.bottleneck(self.pool(e2))\n",
        "        d1 = self.up1(b)\n",
        "        d1 = self.dec1(torch.cat([d1, e2], dim=1))\n",
        "        d2 = self.up2(d1)\n",
        "        d2 = self.dec2(torch.cat([d2, e1], dim=1))\n",
        "        return self.out_conv(d2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1KANxlWTNvVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === LOSS FUNCTIONS ===\n",
        "\n",
        "# Class weights: adjust these based on label distribution\n",
        "class_weights = torch.tensor([0.05, 0.5, 0.8, 1.0]).to(device)\n",
        "\n",
        "def dice_loss(pred, target, smooth=1e-5):\n",
        "    pred = F.softmax(pred, dim=1)\n",
        "    target_onehot = F.one_hot(target, num_classes=pred.shape[1]).permute(0, 4, 1, 2, 3)\n",
        "    target_onehot = target_onehot.float()\n",
        "    intersection = (pred * target_onehot).sum(dim=(2, 3, 4))\n",
        "    union = pred.sum(dim=(2, 3, 4)) + target_onehot.sum(dim=(2, 3, 4))\n",
        "    dice = (2. * intersection + smooth) / (union + smooth)\n",
        "    return 1 - dice.mean()\n",
        "\n",
        "def combined_loss(pred, target):\n",
        "    ce = F.cross_entropy(pred, target, weight=class_weights)\n",
        "    dsc = dice_loss(pred, target)\n",
        "    return ce + dsc\n",
        "\n",
        "# === LOAD FILES ===\n",
        "\n",
        "image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.nii.gz\")))\n",
        "label_paths = sorted(glob.glob(os.path.join(label_dir, \"*.nii.gz\")))\n",
        "\n",
        "# Use a subset for demonstration\n",
        "image_paths = image_paths[:10]\n",
        "label_paths = label_paths[:10]\n",
        "\n",
        "train_dataset = BraTSDataset(image_paths, label_paths)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# === TRAINING LOOP ===\n",
        "\n",
        "model = Simple3DUNet().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = combined_loss(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "YBzEusiKOCko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to save the model\n",
        "save_path = '/content/drive/MyDrive/brats_model_1.pth'  # You can change the filename/path\n",
        "\n",
        "# Save state_dict (model parameters only)\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model saved to {save_path}\")\n"
      ],
      "metadata": {
        "id": "W5xI9tTicOi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# === EVALUATION AND PREDICTION ===\n",
        "\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Pick a sample image from dataset\n",
        "with torch.no_grad():\n",
        "    x_sample, y_true = train_dataset[0]  # Get first sample\n",
        "    x_sample = x_sample.unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    # Predict\n",
        "    logits = model(x_sample)\n",
        "    preds = torch.argmax(F.softmax(logits, dim=1), dim=1).squeeze(0).cpu().numpy()  # shape: (H, W, D)\n",
        "    y_true = y_true.numpy()  # ground truth (H, W, D)\n",
        "\n",
        "# === SLICE-WISE VISUALIZATION ===\n",
        "\n",
        "slice_idx = preds.shape[2] // 2  # Middle axial slice\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "axs[0].imshow(x_sample[0, 0, :, :, slice_idx].cpu(), cmap='gray')\n",
        "axs[0].set_title(\"Input (Modality 1)\")\n",
        "\n",
        "axs[1].imshow(y_true[:, :, slice_idx], cmap='nipy_spectral')\n",
        "axs[1].set_title(\"Ground Truth\")\n",
        "\n",
        "axs[2].imshow(preds[:, :, slice_idx], cmap='nipy_spectral')\n",
        "axs[2].set_title(\"Prediction\")\n",
        "\n",
        "for ax in axs:\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "n89pMCRicCOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "def view_slice(i):\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    axs[0].imshow(x_sample[0, 0, :, :, i].cpu(), cmap='gray')\n",
        "    axs[0].set_title(\"Input\")\n",
        "    axs[1].imshow(y_true[:, :, i], cmap='nipy_spectral')\n",
        "    axs[1].set_title(\"Ground Truth\")\n",
        "    axs[2].imshow(preds[:, :, i], cmap='nipy_spectral')\n",
        "    axs[2].set_title(\"Prediction\")\n",
        "    for ax in axs:\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "slider = widgets.IntSlider(min=0, max=preds.shape[2]-1, step=1, value=preds.shape[2]//2)\n",
        "widgets.interact(view_slice, i=slider)\n"
      ],
      "metadata": {
        "id": "DwWhW59Ddr2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to load model data\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/path_to/model_weights.pth\"))\n"
      ],
      "metadata": {
        "id": "1eiaB7UNac-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPROCESSING FUNCTIONS\n",
        "\n",
        "# to do: normalizatio\n",
        "# crop and back to original padding (capire se ha senso)"
      ],
      "metadata": {
        "id": "g6Am1z3cvee_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === SETUP WITH PREPROCESSING ===\n",
        "\n",
        "import os, glob\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === DATASET CLASS ===\n",
        "\n",
        "class BraTSDataset(Dataset):\n",
        "    def __init__(self, image_paths, label_paths, patch_size=(128, 128, 72)):\n",
        "        self.image_paths = image_paths\n",
        "        self.label_paths = label_paths\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = nib.load(self.image_paths[idx]).get_fdata().astype(np.float32)  # (H, W, D, 4)\n",
        "        lbl = nib.load(self.label_paths[idx]).get_fdata().astype(np.uint8)    # (H, W, D)\n",
        "\n",
        "        # Center crop\n",
        "        H, W, D = lbl.shape\n",
        "        ph, pw, pd = self.patch_size\n",
        "        sh = (H - ph) // 2\n",
        "        sw = (W - pw) // 2\n",
        "        sd = (D - pd) // 2\n",
        "\n",
        "        img = img[sh:sh+ph, sw:sw+pw, sd:sd+pd, :]\n",
        "        lbl = lbl[sh:sh+ph, sw:sw+pw, sd:sd+pd]\n",
        "\n",
        "        # Convert to PyTorch tensors: (C, H, W, D) and (H, W, D)\n",
        "        img = torch.tensor(img).permute(3, 0, 1, 2)\n",
        "        lbl = torch.tensor(lbl, dtype=torch.long)\n",
        "        return img, lbl\n",
        "\n",
        "# === SIMPLE 3D UNET ===\n",
        "\n",
        "class Simple3DUNet(nn.Module):\n",
        "    def __init__(self, in_channels=4, out_classes=4):\n",
        "        super().__init__()\n",
        "        self.enc1 = self.conv_block(in_channels, 16)\n",
        "        self.enc2 = self.conv_block(16, 32)\n",
        "        self.pool = nn.MaxPool3d(2)\n",
        "\n",
        "        self.bottleneck = self.conv_block(32, 64)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)\n",
        "        self.dec1 = self.conv_block(64, 32)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose3d(32, 16, kernel_size=2, stride=2)\n",
        "        self.dec2 = self.conv_block(32, 16)\n",
        "\n",
        "        self.out_conv = nn.Conv3d(16, out_classes, kernel_size=1)\n",
        "\n",
        "    def conv_block(self, in_ch, out_ch):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv3d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool(e1))\n",
        "        b = self.bottleneck(self.pool(e2))\n",
        "        d1 = self.up1(b)\n",
        "        d1 = self.dec1(torch.cat([d1, e2], dim=1))\n",
        "        d2 = self.up2(d1)\n",
        "        d2 = self.dec2(torch.cat([d2, e1], dim=1))\n",
        "        return self.out_conv(d2)"
      ],
      "metadata": {
        "id": "KJLf1hDhuAHb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}